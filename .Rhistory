nancy_choices = ifelse(between(sat_avg_2013, 1490, 1690) & region == 2, TRUE, FALSE),
rei_choices = ifelse(between(sat_avg_2013, 1390, 1590) & pct_socialscience_2000 > 30, TRUE, FALSE),
casey_choices = ifelse(between(sat_avg_2013, 1500, 1700) & ((public == 1 & state == "CA") | (scorecard_netprice_2013 < 10000)), TRUE, FALSE)
)
bff_super_awesome_college_list <-
mrc_data %>%
mutate(abdul_choices = ifelse(between(sat_avg_2013, 1330, 1530) &
(tier_name == "Ivy Plus" | flagship == 1 ), TRUE, FALSE),
stephen_choices = ifelse(between(sat_avg_2013, 1350, 1550) & public == 0, TRUE, FALSE),
nancy_choices = ifelse(between(sat_avg_2013, 1490, 1690) & region == 2, TRUE, FALSE),
rei_choices = ifelse(between(sat_avg_2013, 1390, 1590) & pct_socialscience_2000 > 30, TRUE, FALSE),
casey_choices = ifelse(between(sat_avg_2013, 1500, 1700) & ((public == 1 & state == "CA") | (scorecard_netprice_2013 < 10000)), TRUE, FALSE)
)
View(bff_super_awesome_college_list)
bff_super_awesome_college_list %>%
filter(abdul_choices, sam_choices, nancy_choices, rei_choices, cary_choices)
bff_super_awesome_college_list %>%
filter(abdul_choices, stephen_choices, nancy_choices, rei_choices, cary_choices)
bff_super_awesome_college_list %>%
filter(abdul_choices, stephen_choices, nancy_choices, rei_choices, casey_choices)
bff_super_awesome_college_list %>%
select(abdul_choices, stephen_choices, nancy_choices, rei_choices, casey_choices)
?filter()
bff_super_awesome_college_list %>%
filter(abdul_choices | stephen_choices | nancy_choices | rei_choices | casey_choices)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(haven)
wid_data_raw <-
readxl::read_xlsx("data/world_wealth_inequality.xlsx",
col_names = c("country", "indicator", "percentile", "year", "value")) %>%
separate(indicator, sep = "\\n", into = c("row_tag", "type", "notes"))
wid_data <- wid_data_raw %>%
select(-row_tag) %>%
select(-notes, everything()) %>%
# some students had trouble because excel added "\r" to the end
# of each string. mutate standardizes the string across platforms.
mutate(type = ifelse(str_detect(type, "Net personal wealth"),
"Net personal wealth", type)) %>%
filter(type == "Net personal wealth")
wid_data %>%
filter(country %in% c("China", "India", "USA")) %>%
group_by(country) %>%
summarize(p90_mean = mean(percentile == "p90p100",
na.rm = TRUE))
wid_data %>%
filter(country %in% c("China", "India", "USA")) %>%
group_by(country) %>%
summarize(p90_mean = mean(percentile == "p90p100",
na.rm = TRUE))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
open_policing <- read_csv("data/open_policing_chicago.csv")
View(open_policing)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
open_policing <-
open_policing_raw %>%
select(col_select)
open_policing_raw <-
read_csv("data/open_policing_chicago.csv")
col_select <- c("subject_age", "subject_race", "subject_sex", "officer_age", "officer_race", "officer_sex", "officer_years_of_service", "violation", "arrest_made", "citation_issued", "outcome")
open_policing <-
open_policing_raw %>%
select(col_select)
data_for_plot <-
open_policing %>%
filter(subject_race %in% c("black", "hispanic", "white")) %>%
group_by(subject_race, outcome) %>%
summarize(num_stops = n()) %>%
ungroup() %>%
mutate(percent_total_stops = (num_stops / sum(num_stops))*100)
data_for_plot %>%
ggplot(aes(x = subject_race,
y = percent_total_stops,
fill = outcome)
) +
geom_col() +
labs(y = "Percentage (%) of Total Stops",
x = "Driver's Race",
title = "Racial Disparities in Policing in Chicago From December 2011 - 2016",
subtitle = "Data source: Open Policing - Stanford University") +
theme(legend.position = "bottom")
summary_table <-
open_policing %>%
filter(subject_race %in% c("black", "hispanic", "white")) %>%
group_by(subject_race, outcome) %>%
summarize(num_stops = n()) %>%
ungroup() %>%
mutate(percent_stops = (num_stops / sum(num_stops))*100)
take_n-th_power <- function(num, power = 2) {
num ^ power
}
take_nth_power <- function(num, power = 2) {
num ^ power
}
take_nth_power(6)
take_nth_power(3, 3)
g(y = 2, a = 1, n = 2)
f <- function(x, n){
x^n
}
g <- function(y, a, n){
a + f(y)
}
g(y = 2, a = 1, n = 2)
g(y = 2, a = 1, n = 2)
# You might need some tidy functions later.
library(tidyverse)
get_zscores <- function(obs_mean, obs_sd, true_mean, N) {
zscore <- (obs_mean - true_mean) / (obs_sd / sqrt(N))
return(zscore)
}
test
test <- get_zscores(obs_mean = 4.4, true_mean = 4.3, obs_sd = 0.25, N = 100)
test
made_up_means <- c(4.4, 4.1, 4.2, 4.4, 4.2)
made_up_sd <- c(.25, .5, .4, 1, .4)
made_up_zscores <- get_zscores(obs_mean = made_up_means,
true_mean = 4.3,
obs_sd = made_up_sd,
N = 100)
made_up_zscores
test_significance <- function(zscores, alpha) {
abs(zscores) > qnorm(1 - alpha/2)
}
test_significance(zscores = 2, alpha = 0.05)
test_significance(zscores = c(1.9, -0.3, -3), alpha = 0.05)
get_mean_and_sd_from_random_sample <- function(N, true_mean){
one_simulation <- rnorm(N, mean = true_mean)
sim_mean <- mean(one_simulation)
sim_std_dev <- sd(one_simulation)
data.frame("obs_mean" = sim_mean, "obs_sd" = sim_std_dev)
}
set.seed(5)
get_mean_and_sd_from_random_sample(N = 30, true_mean = 0.5)
set.seed(5)
replicate(n = 3, rnorm(5, 0, 1), simplify = TRUE)
set.seed(5)
replicate(n = 3, get_mean_and_sd_from_random_sample(N = 30, true_mean = 0.5), simplify = TRUE)
set.seed(5)
replicate(n = 2, get_mean_and_sd_from_random_sample(N = 30, true_mean = 0.5), simplify = FALSE)
set.seed(4)
replicate(n = 2, get_mean_and_sd_from_random_sample(N = 30, true_mean = 0.5), simplify = FALSE) %>%
bind_rows()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
open_policing_raw <-
read_csv("data/open_policing_chicago.csv")
View(open_policing_raw)
col_select <- c("subject_age", "subject_race", "subject_sex", "officer_age", "officer_race", "officer_sex", "officer_years_of_service", "violation", "arrest_made", "citation_issued", "outcome")
open_policing <-
open_policing_raw %>%
select(col_select)
View(open_policing)
data_for_plot <-
open_policing %>%
filter(subject_race %in% c("black", "hispanic", "white")) %>%
group_by(subject_race, outcome) %>%
summarize(num_stops = n()) %>%
ungroup() %>%
mutate(percent_total_stops = (num_stops / sum(num_stops))*100)
View(data_for_plot)
library(tidyverse)
?qnorm
?t.test
#HYPOTHESIS TESTING
# find p-value with z-score
pnorm(q = 5.49, mean = 0, sd = 1, lower.tail = F)
# find p-value with t statistic
pt(q = 1.88, df = 24, lower.tail = F)
?arrange()
?arrange()
library(tidyverse)
?arrange()
?geom_density()
library(tidyverse)
?geom_density
?facet_grid
library(tidyverse)
library(tinytex)
getwd()
cps_report_card <-
read_csv("Chicago_Public_Schools_-_Progress_Report_Cards__2011-2012_.csv")
cps_report_card %>%
ggplot(aes(x = `College Eligibility %`, y = `College Enrollment Rate %`)) +
geom_point()
cps_report_card %>%
ggplot(aes(x= `Average Student Attendance`, y= `College Eligibility %`)) +
geom_point()
cps_report_card %>%
ggplot(aes(x = `Graduation Rate %`, y = `College Enrollment Rate %`)) +
geom_point()
getwd()
getwd()
#install.packages("GGally")
library("ggplot2")
library("GGally")
x <- c(1,2,3)
x
# more info about tibbles
vignette("tibble")
# other packages may give you back a base R data.frame
# turn it into a tibble
tibble(mpg)
# create a new tibble from scratch
# notice how for column y, the value 1 is "recycled" (vectorized)
# in column z, you can refer to things you just created, e.g. column x
tibble(
x = 1:5,
y = 1,
z = x ^ 2 + y
)
college <- read_csv("College.csv", skip=0)
# imports
library(tidyverse)
library(nycflights13)
View(planes)
# confirm a primary key
planes %>%
count(tailnum) %>%
filter(n > 1)
weather %>%
count(year, month, day, hour, origin) %>%
filter(n > 1)
# confirm a primary key
planes %>%
count(tailnum)
weather %>%
count(year, month, day, hour, origin) %>%
filter(n > 1)
flights %>%
count(year, month, day, tailnum) %>%
filter(n > 1)
# answer:
flights %>%
arrange(year, month, day, sched_dep_time, carrier, flight) %>%
mutate(flights_ID = row_number()) %>%
glimpse()
install.packages('maps')
library(maps)
avg_dest_delays <-
flights %>%
group_by(dest) %>%
summarise(delay = mean(arr_delay, na.rm = TRUE)) %>%
inner_join(airports, by = c(dest = "faa"))
avg_dest_delays %>%
ggplot(aes(lon, lat, colour = delay)) +
borders("state") +
geom_point() +
coord_quickmap()
# find most common destinations
top_dest <- flights %>%
count(dest, sort = TRUE) %>%
head(10)
View(top_dest)
# find most common destinations
View(flights)
# are there flights that aren't in planes?
flights %>%
anti_join(planes, by = "tailnum") %>%
count(tailnum, sort = TRUE)
library(tidyverse)
library(tidyverse)
data(iris)
# Simple scatterplot
iris_plot <- ggplot(iris, aes(Sepal.Length, Petal.Length, colour=Species)) +
geom_point()
iris_plot
iris_plot + annotate("text", x = 6, y = 5, label = "some text")
iris_plot +
annotate("rect", xmin = 5, xmax = 7, ymin = 4, ymax = 6, alpha = .5)
iris_plot + annotate("segment", x = 5, xend = 7, y = 3, yend = 7, colour = "black")
iris_plot + theme(legend.position="bottom")
iris_plot + theme(legend.text = element_text(colour = "green", size = 8, face = "bold"))
iris_plot + theme(legend.position="bottom") +
theme(legend.text = element_text(colour = "green", size = 8, face = "bold")) +
theme(legend.title = element_text(colour = "blue", size = 10, face = "bold"))
# shape is square
iris_plot + geom_smooth(method=lm) +
geom_point(shape=0)
# shape is triangle
iris_plot + geom_smooth(method=lm) +
geom_point(shape=2)
ggplot(mpg, aes(cyl, hwy)) +
geom_point() +
geom_jitter(aes(colour = class))
view(mpg)
mpg_plot <- ggplot(mpg, aes(x=factor(cyl)))+
geom_bar(stat="count")
View(mpg_plot)
ggplot(data=mpg, aes(x=hwy)) +
geom_histogram( col="red",
fill="green",
alpha = .2,
binwidth = 5)
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy)) +
facet_wrap(~ class, nrow = 2)
# use facet_grid for multiple variables
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy)) +
facet_grid(drv ~ cyl)
# wrapper around base R's modeling fxns to make them work in a pipe
# https://modelr.tidyverse.org/
# (it's already in the tidyverse, but just drawing attention to it today)
library(modelr)
# This command tells R to warn us when doing things like
# dropping rows with missing data!
# The default is for R to drop it silently (no warning)
options(na.action = na.warn)
# use generated data from the modelr package
ggplot(sim1, aes(x, y)) +
geom_point()
# generate some random data for an intercept and slope
models <- tibble(
intr = runif(250, -20, 40),
slope = runif(250, -5, 5)
)
# plot the randomly generated lines
ggplot(sim1, aes(x, y)) +
geom_abline(aes(intercept = intr, slope = slope), data = models, alpha = 1/4) +
geom_point()
# create predictions from slope, intercept and data
predict_ <- function(intr, slope, data) {
intr + data$x * slope
}
View(predict_)
rmse <- function(intr, slope, data) {
diff <- data$y - predict_(intr, slope, data)
sqrt(mean(diff ^ 2))
}
rmse(7, 1.5, sim1)
# measure RMSE for all possible models (lines)
models <- models %>%
mutate(rmse_out = purrr::map2_dbl(intr, slope, rmse, sim1))
?map2_dbl
models
# plot best 10 of above arbitrary lines, colored by RMSE (lower = brighter)
ggplot(sim1, aes(x, y)) +
geom_point(size = 2, colour = "grey30") +
geom_abline(
aes(intercept = intr, slope = slope, colour = -rmse_out),
data = filter(models, rank(rmse_out) <= 10)
)
# fitting a model
sim1_mod <- lm(y ~ x, data = sim1)
summary(sim1_mod)$r.squared
modelr::rmse(sim1_mod, sim1)
df <- tribble( #row-wise tibble creation: tribble
~y, ~x1, ~x2,
4, 2, 5,
5, 1, 6
)
df
model_matrix(df, y ~ x1)
?model_matrix
# with categorical
df_cat <- tribble(
~ sex, ~ y,
"male", 1,
"female", 2,
"male", 1
)
View(df_cat)
df_cat
model_matrix(df_cat, y ~ sex)
###################
######## interactions: continuous and categorical
ggplot(sim3, aes(x1, y)) +
geom_point(aes(colour = x2))
# two possible interactions in the model
mod3a <- lm(y ~ x1 + x2, data = sim3)
mod3b <- lm(y ~ x1 * x2, data = sim3)
grid <- sim3 %>%
data_grid(x1, x2) %>%
gather_predictions(mod3a, mod3b)
grid
# visualize both models
ggplot(sim3, aes(x1, y, colour = x2)) +
geom_point() +
geom_line(data = grid, aes(y = pred)) +
facet_wrap(~ model)
# residuals for each model
sim3 <- sim3 %>%
gather_residuals(mod3a, mod3b)
ggplot(sim3, aes(x1, resid, color = x2)) +
geom_point() +
facet_grid(model ~ x2)
# EDA
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
ggplot(diamonds, aes(color, price)) + geom_boxplot() # J is worst color
ggplot(diamonds, aes(clarity, price)) + geom_boxplot() # I1 is worst clarity
ggplot(diamonds, aes(carat, price)) + # carat is weight of the diamond
geom_hex(bins = 50)
# takeaway: worse quality diamonds have higher price??
install.packages("hexbin")
ggplot(diamonds, aes(carat, price)) + # carat is weight of the diamond
geom_hex(bins = 50)
diamonds2 <- diamonds %>%
filter(carat <= 2.5) %>%
mutate(lprice = log2(price), lcarat = log2(carat))
# replot carat v price: should be easier to see relationship now
ggplot(diamonds2, aes(lcarat, lprice)) +
geom_hex(bins = 50)
# fit linear model
mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)
# compare predictions to actual (remember to undo log transform!)
grid <- diamonds2 %>%
data_grid(carat = seq_range(carat, 20)) %>%
mutate(lcarat = log2(carat)) %>%
add_predictions(mod_diamond, "lprice") %>%
mutate(price = 2 ^ lprice)
ggplot(diamonds2, aes(carat, price)) +
geom_hex(bins = 50) +
geom_line(data = grid, colour = "red", size = 1)
grid
# check residuals (want these to be random! otherwise we've left info on the table that our model could capture)
diamonds2 <- diamonds2 %>%
add_residuals(mod_diamond, "lresid")
ggplot(diamonds2, aes(lcarat, lresid)) +
geom_hex(bins = 50)
# remember, our model predicted price based ONLY ON carat
# these plots show us that as, e.g., color gets worse,
# predicted price was higher than actual. Why?
ggplot(diamonds2, aes(cut, lresid)) + geom_boxplot()
ggplot(diamonds2, aes(color, lresid)) + geom_boxplot()
ggplot(diamonds2, aes(clarity, lresid)) + geom_boxplot()
##### adding vars to the model
mod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2)
# create predictions
grid <- diamonds2 %>%
data_grid(cut, .model = mod_diamond2) %>%
add_predictions(mod_diamond2)
grid
# plot prediction by cut
ggplot(grid, aes(cut, pred)) +
geom_point()
# add residuals
diamonds2 <- diamonds2 %>%
add_residuals(mod_diamond2, "lresid2")
# plot residuals vs lcarat (log carat)
# residual of 2 indicates that the diamond is 4x the predicted price - outliers?
ggplot(diamonds2, aes(lcarat, lresid2)) +
geom_hex(bins = 50)
df <- tribble(
~x, ~y,
1, 2.2,
2, NA,
3, 3.5,
4, 8.3,
NA, 10
)
mod <- lm(y ~ x, data = df)
library(rsample)
# set random seed to make the split repeatable
set.seed(42)
# use the iris dataset in base R
iris_split <- initial_split(iris, prop = 0.6)
train_data <- training(iris_split) # extract the actual data
test_data <- testing(iris_split) # extract the actual data
library(ISLR)
default_data <- ISLR::Default
# check data type of an object in R using class(.)
class(default_data)
# change default_data into a tibble (data frame type object in the tidyverse)
default_tb <- tibble(default_data)
class(default_tb)
summary(default_tb)
default_split <- initial_split(default_tb, prop = 0.7)
default_train <- training(default_split) # extract the actual data
default_test <- testing(default_split) # extract the actual data
# remember that we set up the model inputs using the R formula type
# default, student, balance and income are columns in our data
model <- glm(default~student+balance+income, family="binomial", data=default_train)
options(scipen=999) # disable scientific notation to make summary easier to read
summary(model)
library(caret)
varImp(model)
### use model to predict on new input
new_student <- tibble(balance = c(500,2000), income = 3000, student = "No")
new_student
#predict probability of defaulting
# output is .06 and .10 -- what does this mean?
predict(model, new_student, type="response")
# what data type is the column default?
# what's a factor?
class(default_tb$default)
# install.packages('InformationValue')
library(InformationValue)
# first get predictions for all observations in the TEST set
predicted <- predict(model, default_test, type="response")
# need to change values in $default to 0/1
# use an if/then statement
# ifelse( condition, value if true, value if false)
default_test$default <- ifelse(default_test$default == "Yes", 1, 0)
optimalCutoff(default_test$default, predicted)
confusionMatrix(default_test$default, predicted)
# sensitivity = true positive rate
sensitivity(default_test$default, predicted)
# specificity = true negative rate
specificity(default_test$default, predicted)
# ROC curve
plotROC(default_test$default, predicted)
getwd()
getwd()
setwd('~/Coding Samples/hr-data')
hr <- read_csv('HRDataset_v14.csv')
library(tidyverse)
hr <- read_csv('HRDataset_v14.csv')
setwd()
getwd()
